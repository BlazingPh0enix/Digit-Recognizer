{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the training and testing data\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X[N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "#Creating data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X[N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Getting the cpu or gpu device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"The model is using {device} device\")\n",
    "\n",
    "#Defining the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the optimizer and loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining training and testing functions\n",
    "def train(dataloader, model, loss_fn, optimzer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #Compute the prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1)*len(X)\n",
    "            print(f\"Loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\" Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Loss: 2.302865 [   64/60000]\n",
      "Loss: 0.301382 [ 6464/60000]\n",
      "Loss: 0.361357 [12864/60000]\n",
      "Loss: 0.430134 [19264/60000]\n",
      "Loss: 0.178793 [25664/60000]\n",
      "Loss: 0.341149 [32064/60000]\n",
      "Loss: 0.241165 [38464/60000]\n",
      "Loss: 0.233154 [44864/60000]\n",
      "Loss: 0.286598 [51264/60000]\n",
      "Loss: 0.302745 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.273742\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Loss: 0.108197 [   64/60000]\n",
      "Loss: 0.267546 [ 6464/60000]\n",
      "Loss: 0.235840 [12864/60000]\n",
      "Loss: 0.215688 [19264/60000]\n",
      "Loss: 0.221602 [25664/60000]\n",
      "Loss: 0.301334 [32064/60000]\n",
      "Loss: 0.163739 [38464/60000]\n",
      "Loss: 0.093933 [44864/60000]\n",
      "Loss: 0.277198 [51264/60000]\n",
      "Loss: 0.351567 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.397730\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Loss: 0.272929 [   64/60000]\n",
      "Loss: 0.133575 [ 6464/60000]\n",
      "Loss: 0.252845 [12864/60000]\n",
      "Loss: 0.245125 [19264/60000]\n",
      "Loss: 0.131587 [25664/60000]\n",
      "Loss: 0.208961 [32064/60000]\n",
      "Loss: 0.188031 [38464/60000]\n",
      "Loss: 0.187779 [44864/60000]\n",
      "Loss: 0.286841 [51264/60000]\n",
      "Loss: 0.285375 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.249855\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Loss: 0.147313 [   64/60000]\n",
      "Loss: 0.219723 [ 6464/60000]\n",
      "Loss: 0.222553 [12864/60000]\n",
      "Loss: 0.435646 [19264/60000]\n",
      "Loss: 0.124915 [25664/60000]\n",
      "Loss: 0.097291 [32064/60000]\n",
      "Loss: 0.173509 [38464/60000]\n",
      "Loss: 0.181284 [44864/60000]\n",
      "Loss: 0.276063 [51264/60000]\n",
      "Loss: 0.324605 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.292598\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Loss: 0.186825 [   64/60000]\n",
      "Loss: 0.130110 [ 6464/60000]\n",
      "Loss: 0.307600 [12864/60000]\n",
      "Loss: 0.191176 [19264/60000]\n",
      "Loss: 0.235506 [25664/60000]\n",
      "Loss: 0.175376 [32064/60000]\n",
      "Loss: 0.131395 [38464/60000]\n",
      "Loss: 0.112717 [44864/60000]\n",
      "Loss: 0.241115 [51264/60000]\n",
      "Loss: 0.257261 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.433670\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Loss: 0.352001 [   64/60000]\n",
      "Loss: 0.228660 [ 6464/60000]\n",
      "Loss: 0.342662 [12864/60000]\n",
      "Loss: 0.179444 [19264/60000]\n",
      "Loss: 0.170801 [25664/60000]\n",
      "Loss: 0.108135 [32064/60000]\n",
      "Loss: 0.135779 [38464/60000]\n",
      "Loss: 0.203286 [44864/60000]\n",
      "Loss: 0.194240 [51264/60000]\n",
      "Loss: 0.232632 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.306652\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Loss: 0.147235 [   64/60000]\n",
      "Loss: 0.247188 [ 6464/60000]\n",
      "Loss: 0.221554 [12864/60000]\n",
      "Loss: 0.229543 [19264/60000]\n",
      "Loss: 0.245171 [25664/60000]\n",
      "Loss: 0.142081 [32064/60000]\n",
      "Loss: 0.223548 [38464/60000]\n",
      "Loss: 0.162809 [44864/60000]\n",
      "Loss: 0.330501 [51264/60000]\n",
      "Loss: 0.234655 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.218046\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Loss: 0.159499 [   64/60000]\n",
      "Loss: 0.145071 [ 6464/60000]\n",
      "Loss: 0.275993 [12864/60000]\n",
      "Loss: 0.225397 [19264/60000]\n",
      "Loss: 0.260813 [25664/60000]\n",
      "Loss: 0.081183 [32064/60000]\n",
      "Loss: 0.177868 [38464/60000]\n",
      "Loss: 0.131568 [44864/60000]\n",
      "Loss: 0.281603 [51264/60000]\n",
      "Loss: 0.324134 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.336734\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Loss: 0.214553 [   64/60000]\n",
      "Loss: 0.249691 [ 6464/60000]\n",
      "Loss: 0.164279 [12864/60000]\n",
      "Loss: 0.156260 [19264/60000]\n",
      "Loss: 0.211606 [25664/60000]\n",
      "Loss: 0.154776 [32064/60000]\n",
      "Loss: 0.161738 [38464/60000]\n",
      "Loss: 0.120822 [44864/60000]\n",
      "Loss: 0.220787 [51264/60000]\n",
      "Loss: 0.221719 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.292495\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Loss: 0.174506 [   64/60000]\n",
      "Loss: 0.086251 [ 6464/60000]\n",
      "Loss: 0.255328 [12864/60000]\n",
      "Loss: 0.224840 [19264/60000]\n",
      "Loss: 0.179559 [25664/60000]\n",
      "Loss: 0.175155 [32064/60000]\n",
      "Loss: 0.133680 [38464/60000]\n",
      "Loss: 0.106667 [44864/60000]\n",
      "Loss: 0.214502 [51264/60000]\n",
      "Loss: 0.289496 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.340058\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Loss: 0.228628 [   64/60000]\n",
      "Loss: 0.183014 [ 6464/60000]\n",
      "Loss: 0.307093 [12864/60000]\n",
      "Loss: 0.146225 [19264/60000]\n",
      "Loss: 0.119060 [25664/60000]\n",
      "Loss: 0.155726 [32064/60000]\n",
      "Loss: 0.163099 [38464/60000]\n",
      "Loss: 0.112609 [44864/60000]\n",
      "Loss: 0.159855 [51264/60000]\n",
      "Loss: 0.230381 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.249989\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Loss: 0.086774 [   64/60000]\n",
      "Loss: 0.123033 [ 6464/60000]\n",
      "Loss: 0.233694 [12864/60000]\n",
      "Loss: 0.195549 [19264/60000]\n",
      "Loss: 0.144149 [25664/60000]\n",
      "Loss: 0.200203 [32064/60000]\n",
      "Loss: 0.126637 [38464/60000]\n",
      "Loss: 0.171780 [44864/60000]\n",
      "Loss: 0.319175 [51264/60000]\n",
      "Loss: 0.213157 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.261714\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Loss: 0.154911 [   64/60000]\n",
      "Loss: 0.239712 [ 6464/60000]\n",
      "Loss: 0.188223 [12864/60000]\n",
      "Loss: 0.188177 [19264/60000]\n",
      "Loss: 0.169157 [25664/60000]\n",
      "Loss: 0.199863 [32064/60000]\n",
      "Loss: 0.199337 [38464/60000]\n",
      "Loss: 0.158442 [44864/60000]\n",
      "Loss: 0.207173 [51264/60000]\n",
      "Loss: 0.269254 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.251373\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Loss: 0.151712 [   64/60000]\n",
      "Loss: 0.290468 [ 6464/60000]\n",
      "Loss: 0.175271 [12864/60000]\n",
      "Loss: 0.245767 [19264/60000]\n",
      "Loss: 0.165001 [25664/60000]\n",
      "Loss: 0.309574 [32064/60000]\n",
      "Loss: 0.184341 [38464/60000]\n",
      "Loss: 0.147977 [44864/60000]\n",
      "Loss: 0.213712 [51264/60000]\n",
      "Loss: 0.244012 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.276599\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Loss: 0.226980 [   64/60000]\n",
      "Loss: 0.221340 [ 6464/60000]\n",
      "Loss: 0.147987 [12864/60000]\n",
      "Loss: 0.205858 [19264/60000]\n",
      "Loss: 0.145500 [25664/60000]\n",
      "Loss: 0.310827 [32064/60000]\n",
      "Loss: 0.227255 [38464/60000]\n",
      "Loss: 0.203748 [44864/60000]\n",
      "Loss: 0.418032 [51264/60000]\n",
      "Loss: 0.258063 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.208154\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Loss: 0.121929 [   64/60000]\n",
      "Loss: 0.129489 [ 6464/60000]\n",
      "Loss: 0.135853 [12864/60000]\n",
      "Loss: 0.273807 [19264/60000]\n",
      "Loss: 0.140129 [25664/60000]\n",
      "Loss: 0.224093 [32064/60000]\n",
      "Loss: 0.147178 [38464/60000]\n",
      "Loss: 0.173134 [44864/60000]\n",
      "Loss: 0.161083 [51264/60000]\n",
      "Loss: 0.288215 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.300342\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Loss: 0.134661 [   64/60000]\n",
      "Loss: 0.190345 [ 6464/60000]\n",
      "Loss: 0.185270 [12864/60000]\n",
      "Loss: 0.269512 [19264/60000]\n",
      "Loss: 0.091141 [25664/60000]\n",
      "Loss: 0.144265 [32064/60000]\n",
      "Loss: 0.164182 [38464/60000]\n",
      "Loss: 0.136512 [44864/60000]\n",
      "Loss: 0.297658 [51264/60000]\n",
      "Loss: 0.297845 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.235372\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Loss: 0.169712 [   64/60000]\n",
      "Loss: 0.223310 [ 6464/60000]\n",
      "Loss: 0.142766 [12864/60000]\n",
      "Loss: 0.221103 [19264/60000]\n",
      "Loss: 0.219337 [25664/60000]\n",
      "Loss: 0.259563 [32064/60000]\n",
      "Loss: 0.204926 [38464/60000]\n",
      "Loss: 0.163755 [44864/60000]\n",
      "Loss: 0.268192 [51264/60000]\n",
      "Loss: 0.295043 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.181002\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Loss: 0.136646 [   64/60000]\n",
      "Loss: 0.184994 [ 6464/60000]\n",
      "Loss: 0.160360 [12864/60000]\n",
      "Loss: 0.194218 [19264/60000]\n",
      "Loss: 0.228518 [25664/60000]\n",
      "Loss: 0.177550 [32064/60000]\n",
      "Loss: 0.137365 [38464/60000]\n",
      "Loss: 0.178957 [44864/60000]\n",
      "Loss: 0.178574 [51264/60000]\n",
      "Loss: 0.214470 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.292182\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Loss: 0.176029 [   64/60000]\n",
      "Loss: 0.240796 [ 6464/60000]\n",
      "Loss: 0.212799 [12864/60000]\n",
      "Loss: 0.169111 [19264/60000]\n",
      "Loss: 0.142444 [25664/60000]\n",
      "Loss: 0.156498 [32064/60000]\n",
      "Loss: 0.206964 [38464/60000]\n",
      "Loss: 0.243321 [44864/60000]\n",
      "Loss: 0.219372 [51264/60000]\n",
      "Loss: 0.309100 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.272222\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Loss: 0.113175 [   64/60000]\n",
      "Loss: 0.113817 [ 6464/60000]\n",
      "Loss: 0.175854 [12864/60000]\n",
      "Loss: 0.171588 [19264/60000]\n",
      "Loss: 0.162339 [25664/60000]\n",
      "Loss: 0.308523 [32064/60000]\n",
      "Loss: 0.150231 [38464/60000]\n",
      "Loss: 0.201266 [44864/60000]\n",
      "Loss: 0.209131 [51264/60000]\n",
      "Loss: 0.250417 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.232355\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Loss: 0.122922 [   64/60000]\n",
      "Loss: 0.127119 [ 6464/60000]\n",
      "Loss: 0.165292 [12864/60000]\n",
      "Loss: 0.144940 [19264/60000]\n",
      "Loss: 0.226353 [25664/60000]\n",
      "Loss: 0.327919 [32064/60000]\n",
      "Loss: 0.149297 [38464/60000]\n",
      "Loss: 0.183415 [44864/60000]\n",
      "Loss: 0.244585 [51264/60000]\n",
      "Loss: 0.298171 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.273327\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Loss: 0.142522 [   64/60000]\n",
      "Loss: 0.124981 [ 6464/60000]\n",
      "Loss: 0.167926 [12864/60000]\n",
      "Loss: 0.186902 [19264/60000]\n",
      "Loss: 0.199878 [25664/60000]\n",
      "Loss: 0.156789 [32064/60000]\n",
      "Loss: 0.160066 [38464/60000]\n",
      "Loss: 0.204445 [44864/60000]\n",
      "Loss: 0.277119 [51264/60000]\n",
      "Loss: 0.265779 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.230470\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Loss: 0.143559 [   64/60000]\n",
      "Loss: 0.180434 [ 6464/60000]\n",
      "Loss: 0.271785 [12864/60000]\n",
      "Loss: 0.244320 [19264/60000]\n",
      "Loss: 0.185474 [25664/60000]\n",
      "Loss: 0.213473 [32064/60000]\n",
      "Loss: 0.180369 [38464/60000]\n",
      "Loss: 0.150116 [44864/60000]\n",
      "Loss: 0.264340 [51264/60000]\n",
      "Loss: 0.256315 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.327015\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Loss: 0.248635 [   64/60000]\n",
      "Loss: 0.156263 [ 6464/60000]\n",
      "Loss: 0.196931 [12864/60000]\n",
      "Loss: 0.261525 [19264/60000]\n",
      "Loss: 0.169230 [25664/60000]\n",
      "Loss: 0.196965 [32064/60000]\n",
      "Loss: 0.196669 [38464/60000]\n",
      "Loss: 0.159859 [44864/60000]\n",
      "Loss: 0.184735 [51264/60000]\n",
      "Loss: 0.257141 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.238987\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Loss: 0.103449 [   64/60000]\n",
      "Loss: 0.185919 [ 6464/60000]\n",
      "Loss: 0.196408 [12864/60000]\n",
      "Loss: 0.200908 [19264/60000]\n",
      "Loss: 0.284543 [25664/60000]\n",
      "Loss: 0.134673 [32064/60000]\n",
      "Loss: 0.159615 [38464/60000]\n",
      "Loss: 0.146877 [44864/60000]\n",
      "Loss: 0.228778 [51264/60000]\n",
      "Loss: 0.286122 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.299311\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Loss: 0.181741 [   64/60000]\n",
      "Loss: 0.160761 [ 6464/60000]\n",
      "Loss: 0.161386 [12864/60000]\n",
      "Loss: 0.196857 [19264/60000]\n",
      "Loss: 0.155785 [25664/60000]\n",
      "Loss: 0.214508 [32064/60000]\n",
      "Loss: 0.195054 [38464/60000]\n",
      "Loss: 0.221102 [44864/60000]\n",
      "Loss: 0.141489 [51264/60000]\n",
      "Loss: 0.264250 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.200610\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Loss: 0.118278 [   64/60000]\n",
      "Loss: 0.149806 [ 6464/60000]\n",
      "Loss: 0.231802 [12864/60000]\n",
      "Loss: 0.156581 [19264/60000]\n",
      "Loss: 0.172388 [25664/60000]\n",
      "Loss: 0.177202 [32064/60000]\n",
      "Loss: 0.253489 [38464/60000]\n",
      "Loss: 0.161494 [44864/60000]\n",
      "Loss: 0.374536 [51264/60000]\n",
      "Loss: 0.269992 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.227619\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Loss: 0.175191 [   64/60000]\n",
      "Loss: 0.112395 [ 6464/60000]\n",
      "Loss: 0.191758 [12864/60000]\n",
      "Loss: 0.154897 [19264/60000]\n",
      "Loss: 0.258062 [25664/60000]\n",
      "Loss: 0.163840 [32064/60000]\n",
      "Loss: 0.133837 [38464/60000]\n",
      "Loss: 0.160028 [44864/60000]\n",
      "Loss: 0.278931 [51264/60000]\n",
      "Loss: 0.262521 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.288180\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Loss: 0.199703 [   64/60000]\n",
      "Loss: 0.197303 [ 6464/60000]\n",
      "Loss: 0.243344 [12864/60000]\n",
      "Loss: 0.222590 [19264/60000]\n",
      "Loss: 0.272427 [25664/60000]\n",
      "Loss: 0.145158 [32064/60000]\n",
      "Loss: 0.158066 [38464/60000]\n",
      "Loss: 0.152145 [44864/60000]\n",
      "Loss: 0.205678 [51264/60000]\n",
      "Loss: 0.195623 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.226314\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Loss: 0.097944 [   64/60000]\n",
      "Loss: 0.191699 [ 6464/60000]\n",
      "Loss: 0.175153 [12864/60000]\n",
      "Loss: 0.205502 [19264/60000]\n",
      "Loss: 0.116907 [25664/60000]\n",
      "Loss: 0.153652 [32064/60000]\n",
      "Loss: 0.154758 [38464/60000]\n",
      "Loss: 0.188374 [44864/60000]\n",
      "Loss: 0.239817 [51264/60000]\n",
      "Loss: 0.240177 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.283172\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Loss: 0.108556 [   64/60000]\n",
      "Loss: 0.183414 [ 6464/60000]\n",
      "Loss: 0.168216 [12864/60000]\n",
      "Loss: 0.201986 [19264/60000]\n",
      "Loss: 0.130024 [25664/60000]\n",
      "Loss: 0.155972 [32064/60000]\n",
      "Loss: 0.219068 [38464/60000]\n",
      "Loss: 0.237953 [44864/60000]\n",
      "Loss: 0.226108 [51264/60000]\n",
      "Loss: 0.190692 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.199804\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Loss: 0.141668 [   64/60000]\n",
      "Loss: 0.141757 [ 6464/60000]\n",
      "Loss: 0.208815 [12864/60000]\n",
      "Loss: 0.192083 [19264/60000]\n",
      "Loss: 0.171282 [25664/60000]\n",
      "Loss: 0.202470 [32064/60000]\n",
      "Loss: 0.213985 [38464/60000]\n",
      "Loss: 0.137700 [44864/60000]\n",
      "Loss: 0.221103 [51264/60000]\n",
      "Loss: 0.291713 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.208924\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Loss: 0.118433 [   64/60000]\n",
      "Loss: 0.227972 [ 6464/60000]\n",
      "Loss: 0.139523 [12864/60000]\n",
      "Loss: 0.211938 [19264/60000]\n",
      "Loss: 0.163859 [25664/60000]\n",
      "Loss: 0.104508 [32064/60000]\n",
      "Loss: 0.150629 [38464/60000]\n",
      "Loss: 0.158826 [44864/60000]\n",
      "Loss: 0.315047 [51264/60000]\n",
      "Loss: 0.241484 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.187707\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Loss: 0.140729 [   64/60000]\n",
      "Loss: 0.153334 [ 6464/60000]\n",
      "Loss: 0.216606 [12864/60000]\n",
      "Loss: 0.144120 [19264/60000]\n",
      "Loss: 0.139381 [25664/60000]\n",
      "Loss: 0.161269 [32064/60000]\n",
      "Loss: 0.124308 [38464/60000]\n",
      "Loss: 0.218483 [44864/60000]\n",
      "Loss: 0.243782 [51264/60000]\n",
      "Loss: 0.278930 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.222848\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Loss: 0.135401 [   64/60000]\n",
      "Loss: 0.092901 [ 6464/60000]\n",
      "Loss: 0.231025 [12864/60000]\n",
      "Loss: 0.264766 [19264/60000]\n",
      "Loss: 0.250825 [25664/60000]\n",
      "Loss: 0.151365 [32064/60000]\n",
      "Loss: 0.171253 [38464/60000]\n",
      "Loss: 0.174939 [44864/60000]\n",
      "Loss: 0.205976 [51264/60000]\n",
      "Loss: 0.297052 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.297848\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Loss: 0.224275 [   64/60000]\n",
      "Loss: 0.313537 [ 6464/60000]\n",
      "Loss: 0.257490 [12864/60000]\n",
      "Loss: 0.186215 [19264/60000]\n",
      "Loss: 0.238366 [25664/60000]\n",
      "Loss: 0.206268 [32064/60000]\n",
      "Loss: 0.120347 [38464/60000]\n",
      "Loss: 0.174276 [44864/60000]\n",
      "Loss: 0.138739 [51264/60000]\n",
      "Loss: 0.266430 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.203663\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Loss: 0.100860 [   64/60000]\n",
      "Loss: 0.113976 [ 6464/60000]\n",
      "Loss: 0.238875 [12864/60000]\n",
      "Loss: 0.200596 [19264/60000]\n",
      "Loss: 0.201040 [25664/60000]\n",
      "Loss: 0.188414 [32064/60000]\n",
      "Loss: 0.144373 [38464/60000]\n",
      "Loss: 0.182454 [44864/60000]\n",
      "Loss: 0.225976 [51264/60000]\n",
      "Loss: 0.255824 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.224547\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Loss: 0.157091 [   64/60000]\n",
      "Loss: 0.174421 [ 6464/60000]\n",
      "Loss: 0.173390 [12864/60000]\n",
      "Loss: 0.092282 [19264/60000]\n",
      "Loss: 0.221950 [25664/60000]\n",
      "Loss: 0.192593 [32064/60000]\n",
      "Loss: 0.192260 [38464/60000]\n",
      "Loss: 0.191816 [44864/60000]\n",
      "Loss: 0.213433 [51264/60000]\n",
      "Loss: 0.289589 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.300846\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Loss: 0.198957 [   64/60000]\n",
      "Loss: 0.167393 [ 6464/60000]\n",
      "Loss: 0.145688 [12864/60000]\n",
      "Loss: 0.154753 [19264/60000]\n",
      "Loss: 0.173844 [25664/60000]\n",
      "Loss: 0.150381 [32064/60000]\n",
      "Loss: 0.221214 [38464/60000]\n",
      "Loss: 0.172425 [44864/60000]\n",
      "Loss: 0.236342 [51264/60000]\n",
      "Loss: 0.275683 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.254392\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Loss: 0.134969 [   64/60000]\n",
      "Loss: 0.230251 [ 6464/60000]\n",
      "Loss: 0.160981 [12864/60000]\n",
      "Loss: 0.191610 [19264/60000]\n",
      "Loss: 0.277717 [25664/60000]\n",
      "Loss: 0.138299 [32064/60000]\n",
      "Loss: 0.188900 [38464/60000]\n",
      "Loss: 0.194825 [44864/60000]\n",
      "Loss: 0.286679 [51264/60000]\n",
      "Loss: 0.251697 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.377276\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Loss: 0.189888 [   64/60000]\n",
      "Loss: 0.216038 [ 6464/60000]\n",
      "Loss: 0.228715 [12864/60000]\n",
      "Loss: 0.170850 [19264/60000]\n",
      "Loss: 0.199008 [25664/60000]\n",
      "Loss: 0.261649 [32064/60000]\n",
      "Loss: 0.175336 [38464/60000]\n",
      "Loss: 0.178489 [44864/60000]\n",
      "Loss: 0.250999 [51264/60000]\n",
      "Loss: 0.268311 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.213483\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Loss: 0.154301 [   64/60000]\n",
      "Loss: 0.208971 [ 6464/60000]\n",
      "Loss: 0.283614 [12864/60000]\n",
      "Loss: 0.215455 [19264/60000]\n",
      "Loss: 0.202406 [25664/60000]\n",
      "Loss: 0.176814 [32064/60000]\n",
      "Loss: 0.119772 [38464/60000]\n",
      "Loss: 0.192721 [44864/60000]\n",
      "Loss: 0.220161 [51264/60000]\n",
      "Loss: 0.239333 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.243244\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Loss: 0.146588 [   64/60000]\n",
      "Loss: 0.247641 [ 6464/60000]\n",
      "Loss: 0.177159 [12864/60000]\n",
      "Loss: 0.235336 [19264/60000]\n",
      "Loss: 0.222191 [25664/60000]\n",
      "Loss: 0.173755 [32064/60000]\n",
      "Loss: 0.210420 [38464/60000]\n",
      "Loss: 0.221547 [44864/60000]\n",
      "Loss: 0.330915 [51264/60000]\n",
      "Loss: 0.325795 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.225689\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Loss: 0.124324 [   64/60000]\n",
      "Loss: 0.236473 [ 6464/60000]\n",
      "Loss: 0.165299 [12864/60000]\n",
      "Loss: 0.201871 [19264/60000]\n",
      "Loss: 0.266352 [25664/60000]\n",
      "Loss: 0.050733 [32064/60000]\n",
      "Loss: 0.121299 [38464/60000]\n",
      "Loss: 0.216158 [44864/60000]\n",
      "Loss: 0.201332 [51264/60000]\n",
      "Loss: 0.214042 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.284167\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Loss: 0.169306 [   64/60000]\n",
      "Loss: 0.162275 [ 6464/60000]\n",
      "Loss: 0.188028 [12864/60000]\n",
      "Loss: 0.226904 [19264/60000]\n",
      "Loss: 0.191989 [25664/60000]\n",
      "Loss: 0.168539 [32064/60000]\n",
      "Loss: 0.221977 [38464/60000]\n",
      "Loss: 0.207555 [44864/60000]\n",
      "Loss: 0.205678 [51264/60000]\n",
      "Loss: 0.221882 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.199794\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Loss: 0.116558 [   64/60000]\n",
      "Loss: 0.172125 [ 6464/60000]\n",
      "Loss: 0.164107 [12864/60000]\n",
      "Loss: 0.191413 [19264/60000]\n",
      "Loss: 0.255696 [25664/60000]\n",
      "Loss: 0.230331 [32064/60000]\n",
      "Loss: 0.148973 [38464/60000]\n",
      "Loss: 0.195109 [44864/60000]\n",
      "Loss: 0.176968 [51264/60000]\n",
      "Loss: 0.236307 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.319008\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Loss: 0.231267 [   64/60000]\n",
      "Loss: 0.204649 [ 6464/60000]\n",
      "Loss: 0.242233 [12864/60000]\n",
      "Loss: 0.228953 [19264/60000]\n",
      "Loss: 0.251703 [25664/60000]\n",
      "Loss: 0.227908 [32064/60000]\n",
      "Loss: 0.188767 [38464/60000]\n",
      "Loss: 0.174763 [44864/60000]\n",
      "Loss: 0.149387 [51264/60000]\n",
      "Loss: 0.233719 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.229457\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Loss: 0.133525 [   64/60000]\n",
      "Loss: 0.202444 [ 6464/60000]\n",
      "Loss: 0.182243 [12864/60000]\n",
      "Loss: 0.178850 [19264/60000]\n",
      "Loss: 0.186627 [25664/60000]\n",
      "Loss: 0.129442 [32064/60000]\n",
      "Loss: 0.156387 [38464/60000]\n",
      "Loss: 0.274804 [44864/60000]\n",
      "Loss: 0.242478 [51264/60000]\n",
      "Loss: 0.340365 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.280983\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Loss: 0.107942 [   64/60000]\n",
      "Loss: 0.221926 [ 6464/60000]\n",
      "Loss: 0.262981 [12864/60000]\n",
      "Loss: 0.202503 [19264/60000]\n",
      "Loss: 0.190143 [25664/60000]\n",
      "Loss: 0.139907 [32064/60000]\n",
      "Loss: 0.180426 [38464/60000]\n",
      "Loss: 0.188091 [44864/60000]\n",
      "Loss: 0.185620 [51264/60000]\n",
      "Loss: 0.225703 [57664/60000]\n",
      " Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.255906\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "epochs = 50\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "#Saving the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
